# model configuration
model:
  seed: 0
  checkpoint: null  # load weights from checkpoint
  model_class: vit
  model_params:
    hidden_size: 768
    num_hidden_layers: 12
    num_attention_heads: 12
    intermediate_size: 3072
    hidden_act: "gelu"
    hidden_dropout_prob: 0.0
    attention_probs_dropout_prob: 0.0
    initializer_range: 0.02
    layer_norm_eps: 1.e-12
    image_size: 224 # usually 224
    patch_size: 16 # default is 16, we use large patch size
    num_channels: 3 # 3 for RGB
    qkv_bias: True
    decoder_num_attention_heads: 16
    decoder_hidden_size: 512
    decoder_num_hidden_layers: 8
    decoder_intermediate_size: 2048
    mask_ratio: 0.75 # 0 for no masking, usually 0.75 (MAE)
    norm_pix_loss: False

    embed_size: 768 # projected embedding size, used for contrastive learning
    temp_scale: False # temperature scaling for contrastive loss
    proj_type: "bn" # projection head type, linear (ln) or batchnorm (bn)
    use_whitening: False # use whitening for contrastive loss
    shuffle_group: False # shuffle embeddings for contrastive loss
    var_reg: False # use variance regularization for contrastive loss
    cov_reg: False # use covariance regularization for contrastive loss

    random_init: False # use random initialization instead of pretrained weights
    use_infoNCE: False # use InfoNCE loss
    infoNCE_weight: 0.03 # weight for InfoNCE loss
    use_byol: False # use BYOL loss
    byol_weight: 0.1 # weight for BYOL loss
    use_vreg: False # use variance/covariance regularization for BYOL loss
    var_weight: 0.2 # weight for variance regularization
    cov_weight: 0.1 # weight for covariance regularization
    repr_weight: 0.5 # weight for representation loss
    recon_weight: 1.0 # weight for reconstruction loss, usually 1.0

# training configuration
training:
  seed: 0
  imgaug: default
  train_batch_size: 128    # per GPU
  val_batch_size: 1024
  test_batch_size: 128
  num_epochs: 800
  num_workers: 8  # Number of CPU workers for the DataLoader
  num_gpus: 1
  num_nodes: 1
  # frequency to log training metrics
  log_every_n_steps: 10
  # frequency to log validation metrics
  check_val_every_n_epoch: 5

# optimizer configuration
optimizer:
  type: Adam
  accumulate_grad_batches: 1
  lr: 5.e-5
  wd: 0.05
  warmup_pct: 0.15 # cosine/linear
  gamma: 0.95     # step
  div_factor: 10  # cosine
  scheduler: cosine # step/cosine/linear

# data configuration
data:
  data_dir: /PATH/TO/DATA